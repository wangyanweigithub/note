
<!-- vim-markdown-toc GFM -->

* [linux socket设置阻塞与非阻塞](#linux-socket设置阻塞与非阻塞)
	* [非阻塞IO 和阻塞IO：](#非阻塞io-和阻塞io)
	* [读](#读)
	* [写](#写)
* [为什么IO复用要搭配非阻塞IO和应用层buffer](#为什么io复用要搭配非阻塞io和应用层buffer)
	* [为什么IO多路复用要搭配非阻塞IO](#为什么io多路复用要搭配非阻塞io)
	* [为什么non-blocking网络编程中应用层buffer是必须的](#为什么non-blocking网络编程中应用层buffer是必须的)
* [socket模型](#socket模型)
* [优雅关闭](#优雅关闭)
	* [close引用计数减一](#close引用计数减一)
	* [shutdown 关闭链接，close关闭socket，清除资源](#shutdown-关闭链接close关闭socket清除资源)
* [IO模型](#io模型)
	* [DMA 控制器功能](#dma-控制器功能)
	* [读写](#读写)
	* [IO操作](#io操作)
	* [五种I/O 模型](#五种io-模型)
		* [同步阻塞I/O模型](#同步阻塞io模型)
		* [同步非阻塞I/O模型](#同步非阻塞io模型)
		* [I/O复用模型](#io复用模型)
		* [信号驱动I/O模型](#信号驱动io模型)
		* [异步I/O模型](#异步io模型)
		* [总结](#总结)
	* [AIO](#aio)

<!-- vim-markdown-toc -->
## linux socket设置阻塞与非阻塞
### 非阻塞IO 和阻塞IO：
1. 在网络编程中对于一个网络句柄会遇到阻塞IO 和非阻塞IO 的概念, 这里对于这两种socket 先做一下说明：

2. 基本概念：
	1. 阻塞IO: socket 的阻塞模式意味着必须要做完IO 操作（包括错误）才会返回。

	2. 非阻塞IO: 非阻塞模式下无论操作是否完成都会立刻返回，需要通过其他方式来判断具体操作是否成功。
	(对于connect，accpet操作，通过select判断，对于recv，recvfrom，send，sendto通过返回值+错误码来判断)


3. IO模式设置：

	1. SOCKET: 对于一个socket 是阻塞模式还是非阻塞模式的处理方法::

		- 用fcntl 设置;用F_GETFL获取flags,用F_SETFL设置flags|O_NONBLOCK;
		- 同时，recv,send 时使用非阻塞的方式读取和发送消息，即flags设置为MSG_DONTWAIT

	2. 实现: fcntl 函数可以将一个socket 句柄设置成非阻塞模式: 
	```
	flags = fcntl(sockfd, F_GETFL, 0);                       //获取文件的flags值。

	fcntl(sockfd, F_SETFL, flags | O_NONBLOCK);   //设置成非阻塞模式；

	flags  = fcntl(sockfd,F_GETFL,0);

	fcntl(sockfd,F_SETFL,flags&~O_NONBLOCK);    //设置成阻塞模式；
	```

	3. 并在接收和发送数据时: 将recv, send 函数的最后有一个flag 参数设置成MSG_DONTWAIT

	```
	recv(sockfd, buff, buff_size,MSG_DONTWAIT);     //非阻塞模式的消息发送
	send(scokfd, buff, buff_size, MSG_DONTWAIT);   //非阻塞模式的消息接受
	```

4. 普通文件: 对于文件的阻塞模式还是非阻塞模式

	1. 方法1: open时，使用O_NONBLOCK；

	2. 方法法2: fcntl设置，使用F_SETFL，flags|O_NONBLOCK；

5. 消息队列: 对于消息队列消息的发送与接受::

	1. 非阻塞
		msgsnd(sockfd,msgbuf,msgsize(不包含类型大小),IPC_NOWAIT)

	2. 阻塞:这里怎么用不了解，但是默认不指定IPC_NOWait就是阻塞的。    
		msgrcv(scokfd,msgbuf,msgsize(**),msgtype,IPC_NOWAIT);

### 读
1. 阻塞与非阻塞读的区别:  //阻塞和非阻塞的区别在于没有数据到达的时候是否立刻返回．

2. 读(read/recv/msgrcv):
	<font color=green>读的本质来说其实不能是读,在实际中, 具体的接收数据不是由这些调用来进行,
	是由于系统底层自动完成的。read 也好,recv 也好只负责把数据从底层缓冲copy 到我们指定的位置.</font>

2. 对于读来说(read, 或者recv) : 阻塞情况下: 在阻塞条件下，read/recv/msgrcv的行为::

	1. 如果没有发现数据在网络缓冲中会一直等待，

	2. 当发现有数据的时候会把数据读到用户指定的缓冲区，但是如果这个时候读到的数据量比较少，
	比参数中指定的长度要小，read 并不会一直等待下去，而是立刻返回。

	3. read 的原则: 是数据在不超过指定的长度的时候有多少读多少，
	<font color=red>没有数据就会一直等待。</font>

	4. 所以一般情况下:
		我们读取数据都需要采用循环读的方式读取数据，因为一次read 完毕不能保证读到我们需要长度的数据，
	read 完一次需要判断读到的数据长度再决定是否还需要再次读取。

3. 非阻塞情况下: 在非阻塞的情况下，read 的行为::

	1. 如果发现没有数据就直接返回，

	2. 如果发现有数据那么也是采用有多少读多少的进行处理．

	3. 所以read 完一次需要判断读到的数据长度再决定是否还需要再次读取。

4. 对于读而言: 阻塞和非阻塞的区别在于没有数据到达的时候是否立刻返回．

5. recv 中有一个MSG_WAITALL 的参数::
```recv(sockfd, buff, buff_size, MSG_WAITALL)```

	在正常情况下recv 是会等待直到读取到buff_size 长度的数据，但是这里的WAITALL 也只是尽量读全，
在有中断的情况下recv 还是可能会被打断，造成没有读完指定的buff_size的长度。

	所以即使是采用recv + WAITALL 参数还是要考虑是否需要循环读取的问题，在实验中对于多数情况下recv
(使用了MSG_WAITALL)还是可以读完buff_size，所以相应的性能会比直接read 进行循环读要好一些。

6. 注意: 使用MSG_WAITALL时，sockfd必须处于阻塞模式下，否则不起作用。
	<font color=red>所以MSG_WAITALL不能和MSG_NONBLOCK同时使用。</font>

### 写
1. 阻塞与非阻塞写的区别: 
	1. 写(send/write/msgsnd)::
		写的本质也不是进行发送操作,而是把用户态的数据copy 到系统底层去,然后再由系统进行发送操作,
	send，write返回成功，只表示数据已经copy 到底层缓冲,而不表示数据已经发出,更不能表示对方端口已经
	接收到数据.

2. 对于write(或者send)而言，
	1. 阻塞情况下: 阻塞情况下，write会将数据发送完。(不过可能被中断)
	在阻塞的情况下，是会一直等待，直到write 完，全部的数据再返回．这点行为上与读操作有所不同。

	2. 原因:
		读，究其原因主要是读数据的时候我们并不知道对端到底有没有数据，数据是在什么时候结束发送的，
	如果一直等待就可能会造成死循环，所以并没有去进行这方面的处理；

		写，而对于write, 由于需要写的长度是已知的，所以可以一直再写，直到写完．不过问题是write 是
	可能被打断吗，造成write 一次只write 一部分数据, 所以write 的过程还是需要考虑循环write, 只不过多数
	情况下一次write 调用就可能成功.

3. 非阻塞写的情况下:
	非阻塞写的情况下，是采用可以写多少就写多少的策略．与读不一样的地方在于，有多少读多少是由网络发
送的那一端是否有数据传输到为标准，但是对于可以写多少是由本地的网络堵塞情况为标准的，在网络阻塞严重的
时候，网络层没有足够的内存来进行写操作，这时候就会出现写不成功的情况，阻塞情况下会尽可能(有可能被中断)
等待到数据全部发送完毕，对于非阻塞的情况就是一次写多少算多少,没有中断的情况下也还是会出现write 到一部
分的情况.

## 为什么IO复用要搭配非阻塞IO和应用层buffer
> 本文链接：https://blog.csdn.net/qq_33113661/article/details/88533686

### 为什么IO多路复用要搭配非阻塞IO
- select、poll_wait、epoll_wait返回可读≠read去读的时候能读到。
如果不用非阻塞，程序会永远卡在read上。以上情况可能出现在多进程同时监听一个socket，
只有一个进程可以accept，别的都会block。

- 假如socket的读缓冲区已经有足够多的数据，需要read多次才能读完，如果是非阻塞可以在循环里读取，
不用担心阻塞在read上，等到errno被置为EWOULDBLOCK的时候break，安全返回select。但如果是阻塞IO，
只敢读取一次，因为如果读取没有数据的fd，read会阻塞，无法返回select，这样就只能期待着多次从select返回，
每次只读一次，效率低下。而且，如果是ET模式，还会造成数据无人处理，导致饥饿。

### 为什么non-blocking网络编程中应用层buffer是必须的
1. 首先，multiplex的核心思想是——用一个线程去同时对多个socket连接服务，即甩thread/process资源
（因为传统的模式是一个thread/process只对一个connection服务），而想要做到这一点，thread/process就不能
阻塞在某一个socket的read或write上，所以就要用到非阻塞IO原因见上。那么现在假设你要向一个socket发
送100kb的数据，但是write调用中，操作系统只接受了80kb的数据，原因可能是受制于TCP的流量控制等等，
现在你有两个选择: 
	1. 等——你可以while这个write调用，但你不知道要等多久，这取决于对方什么时候收到之前的报文并且滑动窗
	口，而且这样也浪费了处理别的socket的时间。 

	2. 把剩下的20kb存起来，下次再发，具体一点就是把这20kb保存在这个TCPconnection的output buffer里，
	并且注册POLLOUT事件，这样select下次返回的时候就还会来发送这20kb的数据，也不会影响别的socket的
	监听。至于为什么需要inputbuffer 那是因为TCP是一个没有边界的字节流协议，不可能一个数据报就是
	一个请求。

## socket模型
1. 服务器程序，简单的说就是接收网络数据，处理后并返回结果数据。网络模块是其必不可少的部分，
它本质上就是处理socket的五类事件:
	- accept(客户端接入)
	- connect(连接上服务器)
	- read
	- write
	- error
	
2. socket接口有两种工作模式，一种是阻塞模式，一种是非阻塞模式
	- 阻塞模式通常不会用，因为它有两个弊端：
		- 会阻塞线程，要想处理多个连接就必须要一个连接一个线程，这样线程开销大且复杂度高;
		- read时，如果对端链路异常了，协议栈无法正常得到关闭通知，那么就永远阻塞住了。
		
	- 非阻塞模式现今有两类：
		- 一类是同步事件分离器，以select,epoll为代表
		- 一类是异步io,以iocp为代表。
		
		- 在非阻塞模式的实践上，通常会使用reactor和proactor这两种设计模式。

## 优雅关闭
### close引用计数减一
1. close把描述符的引用计数减1，仅在该计数变为0时关闭套接字。shutdown可以不管引用计数就激发TCP的正常连接终止序列
2. close终止读和写两个方向的数据发送。TCP是全双工的，有时候需要告知对方已经完成了数据传送，即使对方仍有数据要发送给我们。

### shutdown 关闭链接，close关闭socket，清除资源
1. 确切地说，close() / closesocket() 用来关闭套接字，将套接字描述符（或句柄）从内存清除，之后再也不能使用该套接字，
	与C语言中的 fclose() 类似。应用程序关闭套接字后，与该套接字相关的连接和缓存也失去了意义，TCP协议会自动触发关闭连接的操作。

2. shutdown() 用来关闭连接，而不是套接字，不管调用多少次 shutdown()，套接字依然存在，直到调用 close() / closesocket() 将套接字从内存清除。

3. 优雅关闭
	1. 设置SO_LINGER:
		SO_LINGER选项用来设置延迟关闭的时间，等待套接字发送缓冲区中的数据发送完成。
		没有设置该选项时，在调用close()后，在发送完FIN后会立即进行一些清理工作并返回。
		如果设置了SO_LINGER选项，并且等待时间为正值，则在清理之前会等待一段时间。

	2. 调用close之前先调用shutdown关闭连接

4. shutdown：
	1. 定义:

	```
	int shutdown(int sock, int howto); //Linux
	int shutdown(SOCKET s, int howto); //Windows
	```

	2. 参数：
		- SHUT_RD：断开输入流。套接字无法接收数据（即使输入缓冲区收到数据也被抹去），
		无法调用输入相关函数。

		- SHUT_WR：断开输出流。套接字无法发送数据，<font color=red>但如果输出缓冲区中还
		有未传输的数据，则将传递到目标主机。</font>

		- SHUT_RDWR：同时断开 I/O 流。相当于分两次调用 shutdown()，其中一次以 SHUT_RD 为
		参数，另一次以 SHUT_WR 为参数。

## IO模型
### DMA 控制器功能

1. 能向CPU发出系统保持(HOLD)信号，提出总线接管请求；
 
2. 当CPU发出允许接管信号后，负责对总线的控制，进入DMA方式;
 
3. 能对存储器寻址及能修改地址指针，实现对内存的读写；
 
4. 能决定本次DMA传送的字节数，判断DMA传送是否借宿。
 
5. 发出DMA结束信号，使CPU恢复正常工作状态。

### 读写
1. 当一个read操作发生时，它会经历两个阶段：
	- 通过read系统调用向内核发起读请求。
	- 内核向硬件发送读指令，并等待读就绪。
	- DMA把将要读取的数据复制到描述符所指向的内核缓存区中。
	- 内核将数据从内核缓存区拷贝到用户进程空间中。

### IO操作
1. 同步IO
    > 当用户发出IO请求操作之后，内核会去查看要读取的数据是否就绪，如果数据没有就绪，就一直等待。
    > 需要通过用户线程或者内核不断地去轮询数据是否就绪，当数据就绪时，再将数据从内核拷贝到用户空间。 

2. 异步IO
    > 只有IO请求操作的发出是由用户线程来进行的，IO操作的两个阶段都是由内核自动完成，然后发
    > 送通知告知用户线程IO操作已经完成。也就是说在异步IO中，不会对用户线程产生任何阻塞。 

3. 阻塞IO
    > 当用户线程发起一个IO请求操作（以读请求操作为例），内核查看要读取的数据还没就绪，当前线
    > 程被挂起，阻塞等待结果返回。 

4. 非阻塞IO
    > 如果数据没有就绪，则会返回一个标志信息告知用户线程当前要读的数据没有就绪。当前线程在拿到
    > 此次请求结果的过程中，可以做其它事情。

### 五种I/O 模型
#### 同步阻塞I/O模型
![阻塞IO模型](G:\picture\program_graph\阻塞IO模型.png)

#### 同步非阻塞I/O模型
![非阻塞IO模型](G:\picture\program_graph\非阻塞IO模型.png)

#### I/O复用模型
![阻塞IO模型](G:\picture\program_graph\IO复用模型.png)

#### 信号驱动I/O模型
![阻塞IO模型](G:\picture\program_graph\信号驱动IO模型.png)

[信号驱动例子](https://blog.csdn.net/mcheaven/article/details/44303753)

#### 异步I/O模型
![阻塞IO模型](G:\picture\program_graph\异步IO模型.png)

#### 总结
1. 信号驱动I/O和IO多路复用有些相似，都是返回准备好读写的描述符，然后在这个描述符上可以进行阻塞的读写。不同是select是多路复用，而信号驱动是通过注册信号处理程序。
2. 异步IO是真正的异步完成了读，当拷贝数据拷贝到了用户空间后，才返回成功指示，这时已经完成了读了。

###  AIO
1. aiocb结构

        struct aiocb 
        { 
          int aio_fildes;                    /* File desriptor.  */                        涉及到的文件句柄，用户填写 
          int aio_lio_opcode;           /* Operation to be performed.  */   执行操作，此位aio_read()会填写 
          int aio_reqprio;                 /* Request priority offset.  */          优先权等级 
          volatile void *aio_buf;       /* Location of buffer.  */                 buffer位置，用户填写 
          size_t aio_nbytes;            /* Length of transfer.  */                 buffer长度，用户填写 
          struct sigevent aio_sigevent; /* Signal number and value.  */  可以定义信号和call back的过程

          /* Internal members.  */ 
          struct aiocb *__next_prio; 
          int __abs_prio; 
          int __policy; 
          int __error_code; 
          __ssize_t __return_value;

          #ifndef __USE_FILE_OFFSET64 
          __off_t aio_offset;           /* File offset.  */                              操作的文件偏移 
          char __pad[sizeof (__off64_t) - sizeof (__off_t)]; 
          #else 
          __off64_t aio_offset;         /* File offset.  */ 
          #endif 
          char __unused[32]; 
        };

        
        struct sigevent {
           int          sigev_notify; /* 通知方法 */
           int          sigev_signo;  /* 通知信号 */
           union sigval sigev_value;  /* 传递的数据*/
           void       (*sigev_notify_function) (union sigval);/* 用线程的函数 */
           void        *sigev_notify_attributes;/* 通知线程的属性

        };
                
2. 必须验证每次读操作的结果的要求下，这个例子给出了两种方案
    1. wait_aio，即执行了aio_read()后，循环检查aio_return返回，这样的方式相比阻塞型的read()没有多少优势
    2. call_back,即执行aio_read()的同时，定义该操作响应信号和回调函数。而程序可以继续执行下去。
        当本次read()操作有结果时，自动执行call_back_handler函数。 （推荐）
  
1. 参考链接:
[AIO](https://blog.csdn.net/gufenglei/article/details/5692181)
